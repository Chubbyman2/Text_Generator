{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generator_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcK17dAYeK/VjIdPGNTD7f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chubbyman2/Text_Generator/blob/master/Text_Generator_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK-FtdphxuiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using GRU model to generate text after training on a sample text\n",
        "# Sample text used is Shakespeare's King Lear"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaSlQhGjxyUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os \n",
        "import time\n",
        "\n",
        "from keras.layers import Dense, GRU, Embedding\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4vyDe0Qx0AB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9da0e30-3fbe-4755-b905-58fde638d5a4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJn13i1Lx2_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data preparation\n",
        "text_file = \"/content/gdrive/My Drive/king_lear.txt\"\n",
        "\n",
        "with open(text_file, \"r\") as file:\n",
        "  text = file.read()\n",
        "\n",
        "chars = sorted(list(set(text))) # getting all unique chars"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLQcLEWWytrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize the text\n",
        "# Split into 2 dicts - chars to nums, nums to chars\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = np.array(chars)\n",
        "\n",
        "# Turns each char in the text to a numerical value\n",
        "text_as_int = np.array([char_indices[c] for c in text])"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHOidPwHy0aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Max length of input sequence\n",
        "max_len = 100\n",
        "examples_per_epoch = len(text)/(max_len+1)\n",
        "\n",
        "# Create training examples/targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTklMkaQzW-d",
        "colab_type": "text"
      },
      "source": [
        "Note: drop_remainder() is a tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default is False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53DdH4n5zSa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(max_len+1, drop_remainder=True)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOUquB7czeI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each sequence, shift the input over by one to form the target text\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "# map method applies function to each batch\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54BWKSny2Bnq",
        "colab_type": "text"
      },
      "source": [
        "Buffer size to shuffle the dataset \n",
        "\n",
        "(TF data is designed to work with possibly infinite sequences, so it doesn't attempt to shuffle the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkD-747qzkOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16f86d61-f2ac-498b-fa17-8253ae6d7d34"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# It only shuffles 10000 elements\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddNMX-Rwzl_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(chars)\n",
        "\n",
        "embedding_dim = 256"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mrM5KWGzmzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build\n",
        "model = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, batch_input_shape=[BATCH_SIZE, None]),\n",
        "    GRU(1024, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
        "    Dense(vocab_size)\n",
        "])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h10dJjDpzpGT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "641290ff-f5fc-45d3-8830-236feb423e8a"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "  # Check output shape\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 87) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsaaD8BIzpfT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "327123d5-0939-4406-a7ba-e0870d052a6d"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 87)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4660974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-NcCZ8fzugB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile\n",
        "model.compile(optimizer=\"Adam\", loss=loss)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNeI0UjezwB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure checkpoints\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "\n",
        "# Name of checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwuInLEFzw6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7d66c804-cea8-464f-c528-fb0121549bfc"
      },
      "source": [
        "# Train\n",
        "model.fit(dataset, batch_size=BATCH_SIZE, epochs=10, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "28/28 [==============================] - 2s 59ms/step - loss: 4.3412\n",
            "Epoch 2/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 2.9663\n",
            "Epoch 3/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 2.6438\n",
            "Epoch 4/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 2.4012\n",
            "Epoch 5/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 2.2544\n",
            "Epoch 6/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 2.1477\n",
            "Epoch 7/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 2.0555\n",
            "Epoch 8/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 1.9799\n",
            "Epoch 9/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 1.9159\n",
            "Epoch 10/10\n",
            "28/28 [==============================] - 2s 57ms/step - loss: 1.8559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc824f3a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ur0aCLczyaM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a59ecead-8d9c-4e01-f57f-702a322af7d2"
      },
      "source": [
        "# Because of the way the RNN state is passe from timestep to timestep,\n",
        "# Model only accepts a fixed batch size once built\n",
        "# To run with different batch_size, rebuild the model and restore the weights from the checkpoint\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS8Ij_GMzzee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build again with batch size = 1\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "model2 = tf.keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, batch_input_shape=[BATCH_SIZE, None]),\n",
        "    GRU(1024, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
        "    Dense(vocab_size)\n",
        "])"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md9vf-xqz0a1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model2.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EISgkWtdz1sS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generating text using the learned model\n",
        "def generate_text(model, start_string):\n",
        "  num_generate = 600\n",
        "\n",
        "  # Vectorizing string\n",
        "  input_eval = [char_indices[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  # Lower temperatures = more predictable text.\n",
        "  # Higher temperatures = more surprising text\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Batch size = 1 here\n",
        "  model2.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model2(input_eval)\n",
        "\n",
        "    # Remove batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # Using a categorical distribution to predict the returned character\n",
        "    predictions = predictions/temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "    # Pass predicted character along as next input to the model\n",
        "    # along with previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    text_generated.append(indices_char[predicted_id])\n",
        "\n",
        "  return (start_string + \"\".join(text_generated))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ2iU3UHz3Pq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "532cddb3-c79e-495a-cffa-7d728d9325a7"
      },
      "source": [
        "print(generate_text(model2, start_string=\"CORDELIA: \"))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CORDELIA: WOPLUMEB,\n",
            "Glues theerut busty hester.\n",
            "     Will, I him flo art bentorant ny fle may sendioht\n",
            "     Ho theey lett in, all myof Henert?\n",
            "\n",
            "             noter a chtallitn the istonnd abl.\n",
            "     ho soard tleve likster hit livt,\n",
            "     By light sir, of not earster]\n",
            "  Cond. Her meast de freaglds.\n",
            "  Kent. Nomy flerace; dearg.\n",
            "  Lear. I ham uthor a hien,\n",
            "     He rave't brene an mend men you\n",
            "     Ippoor no'd ore hirithilf the the me oof\n",
            "     Bind be thing;\n",
            "     And the mede.- Gloonct, O_, Hall,\n",
            "     Somericest, and grierunt with I\n",
            "         Servee ly.\n",
            "  Knmon. Whe neaver stells this th thas not doyes grert-\n",
            "C\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}